{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4108354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# THERMAL COAL PRICE FORECASTING - DATA EXPLORATION & MODELING\n",
    "# =============================================================================\n",
    "# Project: ML-based forecasting of thermal coal prices using market fundamentals\n",
    "# Models: Ridge Regression, Decision Tree, XGBoost, LSTM\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 200)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb246d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Load daily data\n",
    "df = pd.read_csv('../data/raw/daily_market_data.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nShape: {df.shape[0]} observations x {df.shape[1]} variables\")\n",
    "print(f\"Date range: {df.index.min().date()} to {df.index.max().date()}\")\n",
    "print(f\"Total trading days: {len(df)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VARIABLES & OBSERVATIONS COUNT\")\n",
    "print(\"=\"*70)\n",
    "for col in df.columns:\n",
    "    obs = df[col].notna().sum()\n",
    "    missing = df[col].isna().sum()\n",
    "    pct = (obs / len(df)) * 100\n",
    "    print(f\"{col:<25}: {obs:>5} obs ({pct:>5.1f}%) | Missing: {missing}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASIC STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6666c0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA CLEANING & TARGET VARIABLE SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "# Select columns with good coverage (>90%)\n",
    "min_coverage = 0.90\n",
    "good_columns = [col for col in df.columns if df[col].notna().sum() / len(df) >= min_coverage]\n",
    "\n",
    "print(f\"Columns with >{min_coverage*100:.0f}% coverage: {len(good_columns)}\")\n",
    "print(good_columns)\n",
    "\n",
    "# Create clean dataset\n",
    "df_clean = df[good_columns].copy()\n",
    "\n",
    "# Forward fill small gaps (up to 5 days)\n",
    "df_clean = df_clean.ffill(limit=5)\n",
    "\n",
    "# Drop remaining rows with any NaN\n",
    "df_clean = df_clean.dropna()\n",
    "\n",
    "print(f\"\\nClean dataset shape: {df_clean.shape}\")\n",
    "print(f\"Date range: {df_clean.index.min().date()} to {df_clean.index.max().date()}\")\n",
    "\n",
    "# Define TARGET variable: coal_china_yzcm (China Yanzhou Coal Mining)\n",
    "TARGET = 'coal_china_yzcm'\n",
    "print(f\"\\n*** TARGET VARIABLE: {TARGET} ***\")\n",
    "print(f\"    (China Yanzhou Coal Mining - proxy for thermal coal prices)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cacd736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CALCULATE LOG RETURNS (Required for time-series forecasting)\n",
    "# =============================================================================\n",
    "\n",
    "# Calculate log returns for all columns\n",
    "df_returns = np.log(df_clean / df_clean.shift(1))\n",
    "\n",
    "# Drop first row (NaN from shift)\n",
    "df_returns = df_returns.dropna()\n",
    "\n",
    "# Rename columns to indicate returns\n",
    "df_returns.columns = [f'{col}_ret' for col in df_returns.columns]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOG RETURNS CALCULATED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Shape: {df_returns.shape}\")\n",
    "print(f\"Date range: {df_returns.index.min().date()} to {df_returns.index.max().date()}\")\n",
    "\n",
    "# Define target return\n",
    "TARGET_RET = 'coal_china_yzcm_ret'\n",
    "\n",
    "print(f\"\\nTarget variable: {TARGET_RET}\")\n",
    "print(f\"\\nLog Returns Statistics:\")\n",
    "print(df_returns.describe().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50cb27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE ENGINEERING: LAGS & ROLLING WINDOWS\n",
    "# =============================================================================\n",
    "\n",
    "def create_features(df, target_col, lags=[1, 2, 3, 5, 10, 21], windows=[5, 10, 21, 63]):\n",
    "    \"\"\"\n",
    "    Create lagged features and rolling statistics.\n",
    "    \n",
    "    Lags: 1, 2, 3, 5, 10, 21 days (up to ~1 month)\n",
    "    Windows: 5 (week), 10 (2 weeks), 21 (month), 63 (quarter)\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    # 1. LAGGED FEATURES for all columns\n",
    "    print(\"Creating lagged features...\")\n",
    "    for col in df.columns:\n",
    "        for lag in lags:\n",
    "            result[f'{col}_lag{lag}'] = df[col].shift(lag)\n",
    "    \n",
    "    # 2. ROLLING STATISTICS for target and key variables\n",
    "    print(\"Creating rolling statistics...\")\n",
    "    key_cols = [target_col, 'brent_crude_ret', 'natural_gas_hh_ret', 'usd_index_ret']\n",
    "    \n",
    "    for col in key_cols:\n",
    "        for window in windows:\n",
    "            # Rolling mean\n",
    "            result[f'{col}_ma{window}'] = df[col].rolling(window=window).mean()\n",
    "            # Rolling volatility\n",
    "            result[f'{col}_vol{window}'] = df[col].rolling(window=window).std()\n",
    "    \n",
    "    # 3. TARGET: Next day return (what we're predicting)\n",
    "    result['target'] = df[target_col].shift(-1)\n",
    "    \n",
    "    # 4. TARGET DIRECTION: 1 if positive, 0 if negative\n",
    "    result['target_direction'] = (result['target'] > 0).astype(int)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Create features\n",
    "TARGET_RET = 'coal_china_yzcm_ret'\n",
    "df_features = create_features(df_returns, TARGET_RET)\n",
    "\n",
    "# Drop rows with NaN (from lagging and rolling)\n",
    "df_features = df_features.dropna()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Final dataset shape: {df_features.shape}\")\n",
    "print(f\"Date range: {df_features.index.min().date()} to {df_features.index.max().date()}\")\n",
    "print(f\"Number of features: {df_features.shape[1] - 2}\")  # Exclude target columns\n",
    "print(f\"\\nFeature categories:\")\n",
    "print(f\"  - Original returns: 17\")\n",
    "print(f\"  - Lagged features: {17 * 6}\")\n",
    "print(f\"  - Rolling statistics: {4 * 4 * 2}\")\n",
    "print(f\"\\nTarget variable: 'target' (next-day coal return)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8f34a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAIN / VALIDATION / TEST SPLIT (Chronological - No Leakage)\n",
    "# =============================================================================\n",
    "\n",
    "# Separate features and target\n",
    "feature_cols = [col for col in df_features.columns if col not in ['target', 'target_direction']]\n",
    "X = df_features[feature_cols]\n",
    "y = df_features['target']\n",
    "y_class = df_features['target_direction']\n",
    "\n",
    "# Split ratios: 70% train, 15% validation, 15% test\n",
    "n = len(X)\n",
    "train_end = int(n * 0.70)\n",
    "val_end = int(n * 0.85)\n",
    "\n",
    "# Chronological split\n",
    "X_train, y_train = X.iloc[:train_end], y.iloc[:train_end]\n",
    "X_val, y_val = X.iloc[train_end:val_end], y.iloc[train_end:val_end]\n",
    "X_test, y_test = X.iloc[val_end:], y.iloc[val_end:]\n",
    "\n",
    "# Classification targets\n",
    "y_train_class = y_class.iloc[:train_end]\n",
    "y_val_class = y_class.iloc[train_end:val_end]\n",
    "y_test_class = y_class.iloc[val_end:]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAIN / VALIDATION / TEST SPLIT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTraining set:   {len(X_train):>5} samples ({len(X_train)/n*100:.1f}%)\")\n",
    "print(f\"                {X_train.index.min().date()} to {X_train.index.max().date()}\")\n",
    "print(f\"\\nValidation set: {len(X_val):>5} samples ({len(X_val)/n*100:.1f}%)\")\n",
    "print(f\"                {X_val.index.min().date()} to {X_val.index.max().date()}\")\n",
    "print(f\"\\nTest set:       {len(X_test):>5} samples ({len(X_test)/n*100:.1f}%)\")\n",
    "print(f\"                {X_test.index.min().date()} to {X_test.index.max().date()}\")\n",
    "print(f\"\\nFeatures: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783575e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL 1: RIDGE REGRESSION (Linear Baseline)\n",
    "# =============================================================================\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Ridge with different alphas\n",
    "alphas = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "best_ridge_alpha = None\n",
    "best_ridge_score = float('inf')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL 1: RIDGE REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTuning alpha on validation set:\")\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    val_pred = ridge.predict(X_val_scaled)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    print(f\"  Alpha={alpha:<6}: Val RMSE = {val_rmse:.6f}\")\n",
    "    \n",
    "    if val_rmse < best_ridge_score:\n",
    "        best_ridge_score = val_rmse\n",
    "        best_ridge_alpha = alpha\n",
    "\n",
    "# Final Ridge model\n",
    "ridge_final = Ridge(alpha=best_ridge_alpha)\n",
    "ridge_final.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\n→ Best alpha: {best_ridge_alpha}\")\n",
    "print(f\"→ Best Val RMSE: {best_ridge_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffca132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL 2: DECISION TREE REGRESSOR\n",
    "# =============================================================================\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Hyperparameter grid\n",
    "max_depths = [3, 5, 7, 10]\n",
    "min_samples_leafs = [10, 20, 50]\n",
    "\n",
    "best_dt_score = float('inf')\n",
    "best_dt_params = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL 2: DECISION TREE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTuning hyperparameters on validation set:\")\n",
    "\n",
    "for depth in max_depths:\n",
    "    for min_leaf in min_samples_leafs:\n",
    "        dt = DecisionTreeRegressor(\n",
    "            max_depth=depth,\n",
    "            min_samples_leaf=min_leaf,\n",
    "            random_state=42\n",
    "        )\n",
    "        dt.fit(X_train, y_train)  # No scaling needed for trees\n",
    "        val_pred = dt.predict(X_val)\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "        \n",
    "        if val_rmse < best_dt_score:\n",
    "            best_dt_score = val_rmse\n",
    "            best_dt_params = {'max_depth': depth, 'min_samples_leaf': min_leaf}\n",
    "\n",
    "print(f\"  Best params: {best_dt_params}\")\n",
    "print(f\"  Best Val RMSE: {best_dt_score:.6f}\")\n",
    "\n",
    "# Final Decision Tree model\n",
    "dt_final = DecisionTreeRegressor(**best_dt_params, random_state=42)\n",
    "dt_final.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n→ Decision Tree trained with {best_dt_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a55fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL 3: XGBOOST\n",
    "# =============================================================================\n",
    "import xgboost as xgb\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = [\n",
    "    {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 3},\n",
    "    {'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 3},\n",
    "    {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 5},\n",
    "    {'n_estimators': 200, 'learning_rate': 0.05, 'max_depth': 3},\n",
    "]\n",
    "\n",
    "best_xgb_score = float('inf')\n",
    "best_xgb_params = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL 3: XGBOOST\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTuning hyperparameters on validation set:\")\n",
    "\n",
    "for params in param_grid:\n",
    "    model = xgb.XGBRegressor(**params, random_state=42, verbosity=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    print(f\"  {params}: RMSE = {val_rmse:.6f}\")\n",
    "    \n",
    "    if val_rmse < best_xgb_score:\n",
    "        best_xgb_score = val_rmse\n",
    "        best_xgb_params = params\n",
    "\n",
    "# Final XGBoost model\n",
    "xgb_final = xgb.XGBRegressor(**best_xgb_params, random_state=42, verbosity=0)\n",
    "xgb_final.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n→ Best params: {best_xgb_params}\")\n",
    "print(f\"→ Best Val RMSE: {best_xgb_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL 4: LSTM (Deep Learning)\n",
    "# =============================================================================\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Prepare sequences for LSTM\n",
    "def create_sequences(X, y, seq_length=21):\n",
    "    X_seq, y_seq = [], []\n",
    "    X_arr = X.values if hasattr(X, 'values') else X\n",
    "    y_arr = y.values if hasattr(y, 'values') else y\n",
    "    \n",
    "    for i in range(len(X_arr) - seq_length):\n",
    "        X_seq.append(X_arr[i:i+seq_length])\n",
    "        y_seq.append(y_arr[i+seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL 4: LSTM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create sequences\n",
    "SEQ_LENGTH = 21\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, SEQ_LENGTH)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val, SEQ_LENGTH)\n",
    "\n",
    "print(f\"\\nSequence length: {SEQ_LENGTH} days\")\n",
    "print(f\"Training sequences: {X_train_seq.shape}\")\n",
    "print(f\"Validation sequences: {X_val_seq.shape}\")\n",
    "\n",
    "# Build LSTM model\n",
    "tf.random.set_seed(42)\n",
    "lstm_model = Sequential([\n",
    "    LSTM(50, input_shape=(SEQ_LENGTH, X_train_seq.shape[2]), return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(25),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train with early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "print(\"\\nTraining LSTM...\")\n",
    "history = lstm_model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "val_pred_lstm = lstm_model.predict(X_val_seq, verbose=0)\n",
    "lstm_val_rmse = np.sqrt(mean_squared_error(y_val_seq, val_pred_lstm))\n",
    "\n",
    "print(f\"\\n→ LSTM Val RMSE: {lstm_val_rmse:.6f}\")\n",
    "print(f\"→ Stopped at epoch: {len(history.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8caa279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIX: Use sklearn's GradientBoosting instead of XGBoost\n",
    "# =============================================================================\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL 3: GRADIENT BOOSTING (sklearn)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = [\n",
    "    {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 3},\n",
    "    {'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 3},\n",
    "    {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 5},\n",
    "]\n",
    "\n",
    "best_gb_score = float('inf')\n",
    "best_gb_params = {}\n",
    "\n",
    "print(\"\\nTuning hyperparameters:\")\n",
    "for params in param_grid:\n",
    "    model = GradientBoostingRegressor(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    print(f\"  {params}: RMSE = {val_rmse:.6f}\")\n",
    "    \n",
    "    if val_rmse < best_gb_score:\n",
    "        best_gb_score = val_rmse\n",
    "        best_gb_params = params\n",
    "\n",
    "# Final model\n",
    "gb_final = GradientBoostingRegressor(**best_gb_params, random_state=42)\n",
    "gb_final.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n→ Best params: {best_gb_params}\")\n",
    "print(f\"→ Best Val RMSE: {best_gb_score:.6f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# COMPARE ALL MODELS ON VALIDATION SET\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON (Validation Set)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = {\n",
    "    'Ridge': best_ridge_score,\n",
    "    'Decision Tree': best_dt_score,\n",
    "    'Gradient Boosting': best_gb_score,\n",
    "    'LSTM': lstm_val_rmse\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Model':<20} {'Val RMSE':<12} {'Rank'}\")\n",
    "print(\"-\"*40)\n",
    "for rank, (model, rmse) in enumerate(sorted(results.items(), key=lambda x: x[1]), 1):\n",
    "    print(f\"{model:<20} {rmse:<12.6f} {rank}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb9686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL EVALUATION ON TEST SET\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL TEST SET EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTest period: {X_test.index.min().date()} to {X_test.index.max().date()}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "# Predictions on test set\n",
    "ridge_test_pred = ridge_final.predict(X_test_scaled)\n",
    "dt_test_pred = dt_final.predict(X_test)\n",
    "gb_test_pred = gb_final.predict(X_test)\n",
    "\n",
    "# LSTM test predictions (need sequences)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test, SEQ_LENGTH)\n",
    "lstm_test_pred = lstm_model.predict(X_test_seq, verbose=0).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "def calc_metrics(y_true, y_pred, model_name):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    # Directional accuracy\n",
    "    true_dir = (y_true > 0).astype(int)\n",
    "    pred_dir = (y_pred > 0).astype(int)\n",
    "    if hasattr(true_dir, 'values'):\n",
    "        true_dir = true_dir.values\n",
    "    dir_acc = (true_dir == pred_dir).mean()\n",
    "    \n",
    "    return {'Model': model_name, 'RMSE': rmse, 'MAE': mae, 'Dir_Acc': dir_acc}\n",
    "\n",
    "# Calculate for all models\n",
    "test_results = []\n",
    "test_results.append(calc_metrics(y_test, ridge_test_pred, 'Ridge'))\n",
    "test_results.append(calc_metrics(y_test, dt_test_pred, 'Decision Tree'))\n",
    "test_results.append(calc_metrics(y_test, gb_test_pred, 'Gradient Boosting'))\n",
    "test_results.append(calc_metrics(y_test_seq, lstm_test_pred, 'LSTM'))\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(test_results)\n",
    "results_df = results_df.sort_values('RMSE')\n",
    "results_df['Rank'] = range(1, len(results_df) + 1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{results_df.to_string(index=False)}\")\n",
    "\n",
    "# Baseline comparison (predicting zero - random walk)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, np.zeros(len(y_test))))\n",
    "print(f\"\\n--- Baseline (predict 0): RMSE = {baseline_rmse:.6f} ---\")\n",
    "print(f\"--- Best model beats baseline by: {((baseline_rmse - results_df['RMSE'].min()) / baseline_rmse * 100):.2f}% ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbbde7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE IMPORTANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE IMPORTANCE (Top 20)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get feature importance from Gradient Boosting (best model)\n",
    "feature_importance = pd.Series(\n",
    "    gb_final.feature_importances_, \n",
    "    index=feature_cols\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nGradient Boosting - Top 20 Features:\")\n",
    "print(\"-\"*50)\n",
    "for i, (feat, imp) in enumerate(feature_importance.head(20).items(), 1):\n",
    "    print(f\"{i:>2}. {feat:<40} {imp:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "feature_importance.head(20).plot(kind='barh', ax=ax)\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title('Top 20 Features - Gradient Boosting Model')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/feature_importance.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n→ Feature importance plot saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4ad403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE RESULTS & CREATE SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "# Save processed data\n",
    "df_features.to_csv('../data/processed/features_dataset.csv')\n",
    "print(\"✓ Features dataset saved\")\n",
    "\n",
    "# Save model comparison results\n",
    "results_df.to_csv('../data/processed/model_results.csv', index=False)\n",
    "print(\"✓ Model results saved\")\n",
    "\n",
    "# Create summary report\n",
    "summary = f\"\"\"\n",
    "================================================================================\n",
    "THERMAL COAL PRICE FORECASTING - PROJECT SUMMARY\n",
    "================================================================================\n",
    "\n",
    "PROJECT OVERVIEW\n",
    "----------------\n",
    "Target: Next-day thermal coal return prediction\n",
    "Data: Daily market data (2015-2024)\n",
    "Total observations: {len(df_features)}\n",
    "Features: {len(feature_cols)}\n",
    "\n",
    "DATA SPLIT\n",
    "----------\n",
    "Training:   {len(X_train)} samples (2015-04 to 2022-01)\n",
    "Validation: {len(X_val)} samples (2022-01 to 2023-07)  \n",
    "Test:       {len(X_test)} samples (2023-07 to 2024-12)\n",
    "\n",
    "MODEL PERFORMANCE (Test Set)\n",
    "----------------------------\n",
    "{results_df.to_string(index=False)}\n",
    "\n",
    "Baseline RMSE (predict 0): {baseline_rmse:.6f}\n",
    "\n",
    "KEY FINDINGS\n",
    "------------\n",
    "1. Gradient Boosting achieved best directional accuracy (54.7%)\n",
    "2. Decision Tree achieved lowest RMSE (0.0217)\n",
    "3. All models perform close to random walk baseline (expected for efficient markets)\n",
    "4. Top predictive features: Steel sector, Heating oil, Materials, Natural gas\n",
    "\n",
    "FEATURE IMPORTANCE (Top 5)\n",
    "--------------------------\n",
    "1. steel_slx_ret (10.1%) - Steel industry demand\n",
    "2. heating_oil_ret (5.4%) - Energy complex linkage\n",
    "3. materials_xlb_ret (4.8%) - Industrial demand\n",
    "4. coal_china_yzcm_ret_ma63 (4.0%) - Coal momentum\n",
    "5. coal_china_yzcm_ret (3.7%) - Same-day coal movement\n",
    "\n",
    "PRACTICAL IMPLICATIONS\n",
    "----------------------\n",
    "- Models show modest predictive power for direction (>50%)\n",
    "- Steel and energy markets are leading indicators for coal\n",
    "- China economic indicators are important for coal forecasting\n",
    "- Combining with fundamental analysis could improve results\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary\n",
    "with open('../data/processed/project_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "print(\"✓ Project summary saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49fc581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATIONS: PRICE CHARTS & CORRELATION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "\n",
    "# 1. Coal Price Over Time\n",
    "ax1 = axes[0, 0]\n",
    "df_clean['coal_china_yzcm'].plot(ax=ax1, color='black', linewidth=0.8)\n",
    "ax1.set_title('Thermal Coal Price (China Yanzhou Coal)', fontsize=12)\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_ylabel('Price (HKD)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Energy Prices Comparison\n",
    "ax2 = axes[0, 1]\n",
    "df_clean[['brent_crude', 'wti_crude', 'natural_gas_hh']].plot(ax=ax2, linewidth=0.8)\n",
    "ax2.set_title('Energy Prices Comparison', fontsize=12)\n",
    "ax2.set_xlabel('')\n",
    "ax2.set_ylabel('Price')\n",
    "ax2.legend(['Brent Crude', 'WTI Crude', 'Natural Gas'])\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Coal Returns Distribution\n",
    "ax3 = axes[1, 0]\n",
    "df_returns['coal_china_yzcm_ret'].hist(bins=50, ax=ax3, color='steelblue', edgecolor='white')\n",
    "ax3.axvline(x=0, color='red', linestyle='--', linewidth=1)\n",
    "ax3.set_title('Coal Daily Returns Distribution', fontsize=12)\n",
    "ax3.set_xlabel('Daily Return')\n",
    "ax3.set_ylabel('Frequency')\n",
    "\n",
    "# 4. Rolling Volatility\n",
    "ax4 = axes[1, 1]\n",
    "rolling_vol = df_returns['coal_china_yzcm_ret'].rolling(21).std() * np.sqrt(252) * 100\n",
    "rolling_vol.plot(ax=ax4, color='darkred', linewidth=0.8)\n",
    "ax4.set_title('Coal 21-Day Rolling Volatility (Annualized %)', fontsize=12)\n",
    "ax4.set_xlabel('')\n",
    "ax4.set_ylabel('Volatility (%)')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Cumulative Returns\n",
    "ax5 = axes[2, 0]\n",
    "cumulative_returns = (1 + df_returns['coal_china_yzcm_ret']).cumprod()\n",
    "cumulative_returns.plot(ax=ax5, color='green', linewidth=0.8)\n",
    "ax5.set_title('Cumulative Returns (Coal)', fontsize=12)\n",
    "ax5.set_xlabel('')\n",
    "ax5.set_ylabel('Cumulative Return')\n",
    "ax5.axhline(y=1, color='black', linestyle='--', linewidth=0.5)\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Year-over-Year Returns\n",
    "ax6 = axes[2, 1]\n",
    "yearly_returns = df_returns['coal_china_yzcm_ret'].resample('YE').sum() * 100\n",
    "yearly_returns.plot(kind='bar', ax=ax6, color='teal', edgecolor='white')\n",
    "ax6.set_title('Annual Returns (%)', fontsize=12)\n",
    "ax6.set_xlabel('')\n",
    "ax6.set_ylabel('Return (%)')\n",
    "ax6.set_xticklabels([d.year for d in yearly_returns.index], rotation=45)\n",
    "ax6.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/price_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Price analysis charts saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db310273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CORRELATION HEATMAP\n",
    "# =============================================================================\n",
    "\n",
    "# Calculate correlation matrix for returns\n",
    "corr_matrix = df_returns.corr()\n",
    "\n",
    "# Create heatmap\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "\n",
    "sns.heatmap(\n",
    "    corr_matrix, \n",
    "    annot=True, \n",
    "    fmt='.2f', \n",
    "    cmap='RdBu_r', \n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    annot_kws={'size': 8},\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title('Correlation Matrix: Daily Returns', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/correlation_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print top correlations with coal\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TOP CORRELATIONS WITH COAL RETURNS\")\n",
    "print(\"=\"*50)\n",
    "coal_corr = corr_matrix['coal_china_yzcm_ret'].drop('coal_china_yzcm_ret').sort_values(key=abs, ascending=False)\n",
    "for var, corr in coal_corr.head(10).items():\n",
    "    print(f\"{var:<30}: {corr:>6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276dea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DIFFERENT FORECASTING HORIZONS: WEEKLY & MONTHLY\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "def evaluate_horizon(horizon_name, X_tr, y_tr, X_te, y_te):\n",
    "    \"\"\"Train and evaluate model for a specific horizon.\"\"\"\n",
    "    model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "    model.fit(X_tr, y_tr)\n",
    "    pred = model.predict(X_te)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_te, pred))\n",
    "    mae = mean_absolute_error(y_te, pred)\n",
    "    \n",
    "    # Directional accuracy\n",
    "    true_dir = (y_te.values > 0).astype(int)\n",
    "    pred_dir = (pred > 0).astype(int)\n",
    "    dir_acc = (true_dir == pred_dir).mean()\n",
    "    \n",
    "    return {'Horizon': horizon_name, 'RMSE': rmse, 'MAE': mae, 'Dir_Acc': dir_acc, 'Predictions': pred, 'Actual': y_te}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MULTI-HORIZON FORECASTING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load weekly and monthly data\n",
    "df_weekly = pd.read_csv('../data/raw/weekly_market_data.csv', index_col=0, parse_dates=True)\n",
    "df_monthly = pd.read_csv('../data/raw/monthly_market_data.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "horizon_results = []\n",
    "\n",
    "# --- DAILY (already done, use existing) ---\n",
    "horizon_results.append({\n",
    "    'Horizon': 'Daily (1-day ahead)', \n",
    "    'RMSE': results_df[results_df['Model']=='Gradient Boosting']['RMSE'].values[0],\n",
    "    'MAE': results_df[results_df['Model']=='Gradient Boosting']['MAE'].values[0],\n",
    "    'Dir_Acc': results_df[results_df['Model']=='Gradient Boosting']['Dir_Acc'].values[0]\n",
    "})\n",
    "\n",
    "# --- WEEKLY ---\n",
    "print(\"\\nProcessing WEEKLY data...\")\n",
    "df_weekly_clean = df_weekly.dropna(thresh=int(len(df_weekly)*0.5), axis=1).dropna()\n",
    "if 'coal_china_yzcm' in df_weekly_clean.columns:\n",
    "    # Calculate returns\n",
    "    weekly_returns = np.log(df_weekly_clean / df_weekly_clean.shift(1)).dropna()\n",
    "    \n",
    "    # Create simple lagged features\n",
    "    weekly_features = weekly_returns.copy()\n",
    "    for col in weekly_returns.columns[:5]:  # Top 5 columns\n",
    "        for lag in [1, 2, 4]:\n",
    "            weekly_features[f'{col}_lag{lag}'] = weekly_returns[col].shift(lag)\n",
    "    \n",
    "    weekly_features['target'] = weekly_returns['coal_china_yzcm'].shift(-1)\n",
    "    weekly_features = weekly_features.dropna()\n",
    "    \n",
    "    # Split\n",
    "    n_w = len(weekly_features)\n",
    "    train_w = int(n_w * 0.7)\n",
    "    \n",
    "    X_w = weekly_features.drop('target', axis=1)\n",
    "    y_w = weekly_features['target']\n",
    "    \n",
    "    X_train_w, X_test_w = X_w.iloc[:train_w], X_w.iloc[train_w:]\n",
    "    y_train_w, y_test_w = y_w.iloc[:train_w], y_w.iloc[train_w:]\n",
    "    \n",
    "    result_w = evaluate_horizon('Weekly (1-week ahead)', X_train_w, y_train_w, X_test_w, y_test_w)\n",
    "    horizon_results.append({k: v for k, v in result_w.items() if k != 'Predictions' and k != 'Actual'})\n",
    "    print(f\"  Weekly: {len(weekly_features)} samples, Test: {len(X_test_w)}\")\n",
    "\n",
    "# --- MONTHLY ---\n",
    "print(\"Processing MONTHLY data...\")\n",
    "df_monthly_clean = df_monthly.dropna(thresh=int(len(df_monthly)*0.5), axis=1).dropna()\n",
    "if 'coal_china_yzcm' in df_monthly_clean.columns:\n",
    "    # Calculate returns\n",
    "    monthly_returns = np.log(df_monthly_clean / df_monthly_clean.shift(1)).dropna()\n",
    "    \n",
    "    # Create simple lagged features\n",
    "    monthly_features = monthly_returns.copy()\n",
    "    for col in monthly_returns.columns[:5]:\n",
    "        for lag in [1, 2, 3]:\n",
    "            monthly_features[f'{col}_lag{lag}'] = monthly_returns[col].shift(lag)\n",
    "    \n",
    "    monthly_features['target'] = monthly_returns['coal_china_yzcm'].shift(-1)\n",
    "    monthly_features = monthly_features.dropna()\n",
    "    \n",
    "    # Split\n",
    "    n_m = len(monthly_features)\n",
    "    train_m = int(n_m * 0.7)\n",
    "    \n",
    "    X_m = monthly_features.drop('target', axis=1)\n",
    "    y_m = monthly_features['target']\n",
    "    \n",
    "    X_train_m, X_test_m = X_m.iloc[:train_m], X_m.iloc[train_m:]\n",
    "    y_train_m, y_test_m = y_m.iloc[:train_m], y_m.iloc[train_m:]\n",
    "    \n",
    "    result_m = evaluate_horizon('Monthly (1-month ahead)', X_train_m, y_train_m, X_test_m, y_test_m)\n",
    "    horizon_results.append({k: v for k, v in result_m.items() if k != 'Predictions' and k != 'Actual'})\n",
    "    print(f\"  Monthly: {len(monthly_features)} samples, Test: {len(X_test_m)}\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FORECASTING HORIZON COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "horizon_df = pd.DataFrame(horizon_results)\n",
    "print(f\"\\n{horizon_df.to_string(index=False)}\")\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = range(len(horizon_df))\n",
    "bars = ax.bar(x, horizon_df['Dir_Acc'] * 100, color=['steelblue', 'coral', 'seagreen'])\n",
    "ax.axhline(y=50, color='red', linestyle='--', label='Random (50%)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(horizon_df['Horizon'], rotation=15, ha='right')\n",
    "ax.set_ylabel('Directional Accuracy (%)')\n",
    "ax.set_title('Forecasting Accuracy by Horizon')\n",
    "ax.set_ylim(0, 70)\n",
    "ax.legend()\n",
    "\n",
    "for i, bar in enumerate(bars):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "            f\"{horizon_df['Dir_Acc'].iloc[i]*100:.1f}%\", ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/horizon_comparison.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Horizon comparison saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb0af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# WALK-FORWARD CROSS-VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"WALK-FORWARD CROSS-VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use the full feature dataset\n",
    "X_full = df_features[feature_cols]\n",
    "y_full = df_features['target']\n",
    "\n",
    "# Time Series Split with 5 folds\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Store results for each model\n",
    "models_cv = {\n",
    "    'Ridge': Ridge(alpha=100),\n",
    "    'Decision Tree': DecisionTreeRegressor(max_depth=3, min_samples_leaf=50, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=50, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "}\n",
    "\n",
    "cv_results = {name: {'rmse': [], 'dir_acc': []} for name in models_cv.keys()}\n",
    "\n",
    "print(\"\\nRunning 5-fold walk-forward CV...\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X_full), 1):\n",
    "    X_tr, X_val = X_full.iloc[train_idx], X_full.iloc[val_idx]\n",
    "    y_tr, y_val = y_full.iloc[train_idx], y_full.iloc[val_idx]\n",
    "    \n",
    "    print(f\"\\nFold {fold}: Train {len(X_tr)} | Val {len(X_val)} | Period: {X_val.index.min().date()} to {X_val.index.max().date()}\")\n",
    "    \n",
    "    for name, model in models_cv.items():\n",
    "        # Scale for Ridge\n",
    "        if name == 'Ridge':\n",
    "            scaler_cv = StandardScaler()\n",
    "            X_tr_scaled = scaler_cv.fit_transform(X_tr)\n",
    "            X_val_scaled = scaler_cv.transform(X_val)\n",
    "            model.fit(X_tr_scaled, y_tr)\n",
    "            pred = model.predict(X_val_scaled)\n",
    "        else:\n",
    "            model.fit(X_tr, y_tr)\n",
    "            pred = model.predict(X_val)\n",
    "        \n",
    "        # Metrics\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "        true_dir = (y_val.values > 0).astype(int)\n",
    "        pred_dir = (pred > 0).astype(int)\n",
    "        dir_acc = (true_dir == pred_dir).mean()\n",
    "        \n",
    "        cv_results[name]['rmse'].append(rmse)\n",
    "        cv_results[name]['dir_acc'].append(dir_acc)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WALK-FORWARD CV RESULTS (Mean ± Std)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cv_summary = []\n",
    "for name in models_cv.keys():\n",
    "    rmse_mean = np.mean(cv_results[name]['rmse'])\n",
    "    rmse_std = np.std(cv_results[name]['rmse'])\n",
    "    dir_mean = np.mean(cv_results[name]['dir_acc'])\n",
    "    dir_std = np.std(cv_results[name]['dir_acc'])\n",
    "    cv_summary.append({\n",
    "        'Model': name,\n",
    "        'RMSE_Mean': rmse_mean,\n",
    "        'RMSE_Std': rmse_std,\n",
    "        'DirAcc_Mean': dir_mean,\n",
    "        'DirAcc_Std': dir_std\n",
    "    })\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  RMSE:     {rmse_mean:.6f} ± {rmse_std:.6f}\")\n",
    "    print(f\"  Dir Acc:  {dir_mean*100:.2f}% ± {dir_std*100:.2f}%\")\n",
    "\n",
    "# Plot CV results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# RMSE across folds\n",
    "ax1 = axes[0]\n",
    "for name in models_cv.keys():\n",
    "    ax1.plot(range(1, 6), cv_results[name]['rmse'], marker='o', label=name)\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('RMSE')\n",
    "ax1.set_title('Walk-Forward CV: RMSE by Fold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Directional Accuracy across folds\n",
    "ax2 = axes[1]\n",
    "for name in models_cv.keys():\n",
    "    ax2.plot(range(1, 6), [x*100 for x in cv_results[name]['dir_acc']], marker='o', label=name)\n",
    "ax2.axhline(y=50, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "ax2.set_xlabel('Fold')\n",
    "ax2.set_ylabel('Directional Accuracy (%)')\n",
    "ax2.set_title('Walk-Forward CV: Directional Accuracy by Fold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/walkforward_cv.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Walk-forward CV results saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a268ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRADING BACKTEST SIMULATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRADING BACKTEST SIMULATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use test set predictions from Gradient Boosting (best model)\n",
    "test_dates = X_test.index\n",
    "test_actual = y_test.values\n",
    "test_pred = gb_final.predict(X_test)\n",
    "\n",
    "# Trading Strategy: Go long if predicted return > 0, else stay flat (no shorting)\n",
    "# Assume we can trade the coal ETF/stock\n",
    "\n",
    "class TradingBacktest:\n",
    "    def __init__(self, actual_returns, predicted_returns, dates, initial_capital=10000):\n",
    "        self.actual = actual_returns\n",
    "        self.predicted = predicted_returns\n",
    "        self.dates = dates\n",
    "        self.initial_capital = initial_capital\n",
    "        \n",
    "    def run_strategy(self, strategy='long_only'):\n",
    "        \"\"\"\n",
    "        Strategies:\n",
    "        - 'long_only': Go long when prediction > 0, else cash\n",
    "        - 'long_short': Go long when prediction > 0, short when < 0\n",
    "        - 'buy_hold': Always long (benchmark)\n",
    "        \"\"\"\n",
    "        n = len(self.actual)\n",
    "        \n",
    "        # Position: 1 = long, -1 = short, 0 = cash\n",
    "        if strategy == 'long_only':\n",
    "            positions = np.where(self.predicted > 0, 1, 0)\n",
    "        elif strategy == 'long_short':\n",
    "            positions = np.where(self.predicted > 0, 1, -1)\n",
    "        else:  # buy_hold\n",
    "            positions = np.ones(n)\n",
    "        \n",
    "        # Strategy returns\n",
    "        strategy_returns = positions * self.actual\n",
    "        \n",
    "        # Cumulative returns\n",
    "        cumulative = (1 + strategy_returns).cumprod()\n",
    "        \n",
    "        # Metrics\n",
    "        total_return = cumulative[-1] - 1\n",
    "        annual_return = (1 + total_return) ** (252 / n) - 1\n",
    "        volatility = np.std(strategy_returns) * np.sqrt(252)\n",
    "        sharpe = annual_return / volatility if volatility > 0 else 0\n",
    "        \n",
    "        # Max drawdown\n",
    "        peak = np.maximum.accumulate(cumulative)\n",
    "        drawdown = (cumulative - peak) / peak\n",
    "        max_drawdown = np.min(drawdown)\n",
    "        \n",
    "        # Win rate\n",
    "        winning_trades = np.sum((strategy_returns > 0) & (positions != 0))\n",
    "        total_trades = np.sum(positions != 0)\n",
    "        win_rate = winning_trades / total_trades if total_trades > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'strategy': strategy,\n",
    "            'total_return': total_return,\n",
    "            'annual_return': annual_return,\n",
    "            'volatility': volatility,\n",
    "            'sharpe_ratio': sharpe,\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'win_rate': win_rate,\n",
    "            'n_trades': total_trades,\n",
    "            'cumulative': cumulative,\n",
    "            'positions': positions\n",
    "        }\n",
    "\n",
    "# Run backtest\n",
    "backtest = TradingBacktest(test_actual, test_pred, test_dates)\n",
    "\n",
    "strategies = ['buy_hold', 'long_only', 'long_short']\n",
    "backtest_results = {}\n",
    "\n",
    "print(f\"\\nBacktest Period: {test_dates.min().date()} to {test_dates.max().date()}\")\n",
    "print(f\"Number of trading days: {len(test_dates)}\")\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "for strat in strategies:\n",
    "    result = backtest.run_strategy(strat)\n",
    "    backtest_results[strat] = result\n",
    "    \n",
    "    print(f\"\\n{strat.upper().replace('_', ' ')} STRATEGY:\")\n",
    "    print(f\"  Total Return:    {result['total_return']*100:>8.2f}%\")\n",
    "    print(f\"  Annual Return:   {result['annual_return']*100:>8.2f}%\")\n",
    "    print(f\"  Volatility:      {result['volatility']*100:>8.2f}%\")\n",
    "    print(f\"  Sharpe Ratio:    {result['sharpe_ratio']:>8.2f}\")\n",
    "    print(f\"  Max Drawdown:    {result['max_drawdown']*100:>8.2f}%\")\n",
    "    print(f\"  Win Rate:        {result['win_rate']*100:>8.2f}%\")\n",
    "    print(f\"  # Trades:        {result['n_trades']:>8}\")\n",
    "\n",
    "# Plot equity curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Equity Curves\n",
    "ax1 = axes[0, 0]\n",
    "for strat, result in backtest_results.items():\n",
    "    label = strat.replace('_', ' ').title()\n",
    "    ax1.plot(test_dates, result['cumulative'] * 10000, label=label, linewidth=1.5)\n",
    "ax1.axhline(y=10000, color='black', linestyle='--', alpha=0.3)\n",
    "ax1.set_title('Equity Curves (Starting Capital: $10,000)', fontsize=12)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Portfolio Value ($)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Strategy Comparison Bar Chart\n",
    "ax2 = axes[0, 1]\n",
    "metrics = ['total_return', 'sharpe_ratio', 'win_rate']\n",
    "x = np.arange(len(strategies))\n",
    "width = 0.25\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = [backtest_results[s][metric] * (100 if metric != 'sharpe_ratio' else 1) for s in strategies]\n",
    "    ax2.bar(x + i*width, values, width, label=metric.replace('_', ' ').title())\n",
    "\n",
    "ax2.set_xticks(x + width)\n",
    "ax2.set_xticklabels([s.replace('_', ' ').title() for s in strategies])\n",
    "ax2.set_title('Strategy Metrics Comparison')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Monthly Returns Heatmap for Long-Only Strategy\n",
    "ax3 = axes[1, 0]\n",
    "long_only_returns = backtest_results['long_only']['positions'] * test_actual\n",
    "monthly_returns = pd.Series(long_only_returns, index=test_dates).resample('ME').sum() * 100\n",
    "monthly_returns.plot(kind='bar', ax=ax3, color=np.where(monthly_returns > 0, 'green', 'red'))\n",
    "ax3.set_title('Monthly Returns - Long Only Strategy (%)')\n",
    "ax3.set_xlabel('')\n",
    "ax3.axhline(y=0, color='black', linewidth=0.5)\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Position Distribution\n",
    "ax4 = axes[1, 1]\n",
    "positions_long_only = backtest_results['long_only']['positions']\n",
    "pos_counts = pd.Series(positions_long_only).value_counts().sort_index()\n",
    "colors = ['gray' if x == 0 else 'green' for x in pos_counts.index]\n",
    "ax4.bar(['Cash (0)', 'Long (1)'][:len(pos_counts)], pos_counts.values, color=colors[:len(pos_counts)])\n",
    "ax4.set_title('Position Distribution - Long Only Strategy')\n",
    "ax4.set_ylabel('Number of Days')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/trading_backtest.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Trading backtest results saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8f2e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OVERFITTING ANALYSIS: Train vs Validation vs Test\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"OVERFITTING ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate predictions on all sets for each model\n",
    "overfitting_results = []\n",
    "\n",
    "# Ridge\n",
    "ridge_train_pred = ridge_final.predict(X_train_scaled)\n",
    "ridge_val_pred = ridge_final.predict(X_val_scaled)\n",
    "ridge_test_pred = ridge_final.predict(X_test_scaled)\n",
    "\n",
    "overfitting_results.append({\n",
    "    'Model': 'Ridge',\n",
    "    'Train_RMSE': np.sqrt(mean_squared_error(y_train, ridge_train_pred)),\n",
    "    'Val_RMSE': np.sqrt(mean_squared_error(y_val, ridge_val_pred)),\n",
    "    'Test_RMSE': np.sqrt(mean_squared_error(y_test, ridge_test_pred))\n",
    "})\n",
    "\n",
    "# Decision Tree\n",
    "dt_train_pred = dt_final.predict(X_train)\n",
    "dt_val_pred = dt_final.predict(X_val)\n",
    "dt_test_pred = dt_final.predict(X_test)\n",
    "\n",
    "overfitting_results.append({\n",
    "    'Model': 'Decision Tree',\n",
    "    'Train_RMSE': np.sqrt(mean_squared_error(y_train, dt_train_pred)),\n",
    "    'Val_RMSE': np.sqrt(mean_squared_error(y_val, dt_val_pred)),\n",
    "    'Test_RMSE': np.sqrt(mean_squared_error(y_test, dt_test_pred))\n",
    "})\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_train_pred = gb_final.predict(X_train)\n",
    "gb_val_pred = gb_final.predict(X_val)\n",
    "gb_test_pred = gb_final.predict(X_test)\n",
    "\n",
    "overfitting_results.append({\n",
    "    'Model': 'Gradient Boosting',\n",
    "    'Train_RMSE': np.sqrt(mean_squared_error(y_train, gb_train_pred)),\n",
    "    'Val_RMSE': np.sqrt(mean_squared_error(y_val, gb_val_pred)),\n",
    "    'Test_RMSE': np.sqrt(mean_squared_error(y_test, gb_test_pred))\n",
    "})\n",
    "\n",
    "# LSTM (using sequences)\n",
    "lstm_train_pred = lstm_model.predict(X_train_seq, verbose=0).flatten()\n",
    "lstm_val_pred_ov = lstm_model.predict(X_val_seq, verbose=0).flatten()\n",
    "lstm_test_pred_ov = lstm_model.predict(X_test_seq, verbose=0).flatten()\n",
    "\n",
    "overfitting_results.append({\n",
    "    'Model': 'LSTM',\n",
    "    'Train_RMSE': np.sqrt(mean_squared_error(y_train_seq, lstm_train_pred)),\n",
    "    'Val_RMSE': np.sqrt(mean_squared_error(y_val_seq, lstm_val_pred_ov)),\n",
    "    'Test_RMSE': np.sqrt(mean_squared_error(y_test_seq, lstm_test_pred_ov))\n",
    "})\n",
    "\n",
    "# Create DataFrame\n",
    "overfit_df = pd.DataFrame(overfitting_results)\n",
    "overfit_df['Overfit_Ratio'] = overfit_df['Test_RMSE'] / overfit_df['Train_RMSE']\n",
    "overfit_df['Status'] = overfit_df['Overfit_Ratio'].apply(\n",
    "    lambda x: '✓ Good' if x < 1.2 else '⚠ Mild Overfit' if x < 1.5 else '✗ Overfit'\n",
    ")\n",
    "\n",
    "print(\"\\nTrain vs Validation vs Test RMSE:\")\n",
    "print(\"-\"*70)\n",
    "print(overfit_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Overfit Ratio = Test_RMSE / Train_RMSE\n",
    "- Ratio < 1.2: Good generalization\n",
    "- Ratio 1.2-1.5: Mild overfitting  \n",
    "- Ratio > 1.5: Significant overfitting\n",
    "\n",
    "A ratio close to 1.0 means the model generalizes well to unseen data.\n",
    "\"\"\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(overfit_df))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, overfit_df['Train_RMSE'], width, label='Train', color='steelblue')\n",
    "bars2 = ax.bar(x, overfit_df['Val_RMSE'], width, label='Validation', color='coral')\n",
    "bars3 = ax.bar(x + width, overfit_df['Test_RMSE'], width, label='Test', color='seagreen')\n",
    "\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Overfitting Analysis: Train vs Validation vs Test RMSE')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(overfit_df['Model'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add ratio annotations\n",
    "for i, ratio in enumerate(overfit_df['Overfit_Ratio']):\n",
    "    ax.annotate(f'Ratio: {ratio:.2f}', xy=(i, overfit_df['Test_RMSE'].iloc[i]), \n",
    "                xytext=(0, 5), textcoords='offset points', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/overfitting_analysis.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Overfitting analysis saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eb2aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UNSUPERVISED LEARNING: PCA AND K-MEANS CLUSTERING\n",
    "# =============================================================================\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy import stats\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"UNSUPERVISED LEARNING ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare data\n",
    "X_unsupervised = df_returns.dropna()\n",
    "scaler_unsup = StandardScaler()\n",
    "X_scaled = scaler_unsup.fit_transform(X_unsupervised)\n",
    "\n",
    "# =============================================================================\n",
    "# PCA\n",
    "# =============================================================================\n",
    "print(\"\\n--- PRINCIPAL COMPONENT ANALYSIS ---\")\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(X_scaled)\n",
    "\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "cumulative_var = np.cumsum(explained_var)\n",
    "\n",
    "print(\"\\nVariance Explained:\")\n",
    "for i in range(5):\n",
    "    print(f\"  PC{i+1}: {explained_var[i]*100:.1f}% (Cumulative: {cumulative_var[i]*100:.1f}%)\")\n",
    "print(f\"\\n→ First 3 PCs explain {cumulative_var[2]*100:.1f}% of variance\")\n",
    "\n",
    "# =============================================================================\n",
    "# K-MEANS CLUSTERING\n",
    "# =============================================================================\n",
    "print(\"\\n--- K-MEANS CLUSTERING ---\")\n",
    "\n",
    "# Find optimal k\n",
    "inertias, silhouettes = [], []\n",
    "for k in range(2, 8):\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    km.fit(X_scaled)\n",
    "    inertias.append(km.inertia_)\n",
    "    silhouettes.append(silhouette_score(X_scaled, km.labels_))\n",
    "\n",
    "# Fit with k=4\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Analyze clusters\n",
    "print(\"\\nCluster Characteristics:\")\n",
    "for i in range(4):\n",
    "    mask = clusters == i\n",
    "    ret = X_unsupervised.loc[mask, 'coal_china_yzcm_ret'].mean() * 100\n",
    "    vol = X_unsupervised.loc[mask, 'coal_china_yzcm_ret'].std() * 100\n",
    "    pct = mask.sum() / len(clusters) * 100\n",
    "    print(f\"  Cluster {i}: Mean={ret:+.3f}%, Vol={vol:.2f}%, Days={pct:.1f}%\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "axes[0].bar(range(1,6), explained_var[:5]*100); axes[0].set_title('PCA Variance')\n",
    "axes[1].plot(range(2,8), inertias, 'o-'); axes[1].set_title('Elbow Method')\n",
    "colors = ['g','r','b','orange']\n",
    "for i in range(4):\n",
    "    axes[2].scatter(pca_result[clusters==i,0], pca_result[clusters==i,1], c=colors[i], s=5, alpha=0.5)\n",
    "axes[2].set_title('Clusters in PCA Space')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/unsupervised_analysis.png', dpi=150)\n",
    "plt.show()\n",
    "print(\"\\n✓ Unsupervised analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145324b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STATIONARITY TESTS (ADF - Augmented Dickey-Fuller)\n",
    "# =============================================================================\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STATIONARITY ANALYSIS (Augmented Dickey-Fuller Test)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nH0: Series has unit root (non-stationary)\")\n",
    "print(\"H1: Series is stationary\")\n",
    "print(\"If p-value < 0.05, reject H0 → series is stationary\\n\")\n",
    "\n",
    "# Test both prices and returns for key variables\n",
    "test_vars = ['coal_china_yzcm', 'brent_crude', 'natural_gas_hh', 'china_etf_fxi']\n",
    "\n",
    "adf_results = []\n",
    "\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Variable':<25} {'Type':<10} {'ADF Stat':<12} {'p-value':<12} {'Stationary?'}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for var in test_vars:\n",
    "    # Test prices (should be non-stationary)\n",
    "    if var in df.columns:\n",
    "        price_series = df[var].dropna()\n",
    "        adf_price = adfuller(price_series, autolag='AIC')\n",
    "        stationary_price = \"Yes\" if adf_price[1] < 0.05 else \"No\"\n",
    "        print(f\"{var:<25} {'Price':<10} {adf_price[0]:<12.4f} {adf_price[1]:<12.4f} {stationary_price}\")\n",
    "        adf_results.append({'Variable': var, 'Type': 'Price', 'ADF': adf_price[0], 'p-value': adf_price[1], 'Stationary': stationary_price})\n",
    "    \n",
    "    # Test returns (should be stationary)\n",
    "    ret_var = f\"{var}_ret\" if not var.endswith('_ret') else var\n",
    "    if ret_var in df_returns.columns:\n",
    "        return_series = df_returns[ret_var].dropna()\n",
    "        adf_return = adfuller(return_series, autolag='AIC')\n",
    "        stationary_ret = \"Yes\" if adf_return[1] < 0.05 else \"No\"\n",
    "        print(f\"{var:<25} {'Return':<10} {adf_return[0]:<12.4f} {adf_return[1]:<12.4f} {stationary_ret}\")\n",
    "        adf_results.append({'Variable': var, 'Type': 'Return', 'ADF': adf_return[0], 'p-value': adf_return[1], 'Stationary': stationary_ret})\n",
    "\n",
    "print(\"-\"*70)\n",
    "print(\"\\n✓ Key Finding: Prices are non-stationary, returns are stationary\")\n",
    "print(\"  → This justifies our use of log returns for modeling\")\n",
    "\n",
    "# Save results\n",
    "adf_df = pd.DataFrame(adf_results)\n",
    "adf_df.to_csv('../data/processed/adf_test_results.csv', index=False)\n",
    "print(\"\\n✓ ADF results saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a7559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ACF/PACF ANALYSIS (Autocorrelation)\n",
    "# =============================================================================\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AUTOCORRELATION ANALYSIS (ACF/PACF)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Coal returns\n",
    "coal_returns = df_returns['coal_china_yzcm_ret'].dropna()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# ACF of returns\n",
    "plot_acf(coal_returns, lags=40, ax=axes[0, 0], title='ACF of Coal Returns')\n",
    "axes[0, 0].set_xlabel('Lag (days)')\n",
    "axes[0, 0].set_ylabel('Autocorrelation')\n",
    "\n",
    "# PACF of returns\n",
    "plot_pacf(coal_returns, lags=40, ax=axes[0, 1], title='PACF of Coal Returns', method='ywm')\n",
    "axes[0, 1].set_xlabel('Lag (days)')\n",
    "axes[0, 1].set_ylabel('Partial Autocorrelation')\n",
    "\n",
    "# ACF of squared returns (volatility clustering)\n",
    "plot_acf(coal_returns**2, lags=40, ax=axes[1, 0], title='ACF of Squared Returns (Volatility)')\n",
    "axes[1, 0].set_xlabel('Lag (days)')\n",
    "axes[1, 0].set_ylabel('Autocorrelation')\n",
    "\n",
    "# ACF of absolute returns\n",
    "plot_acf(np.abs(coal_returns), lags=40, ax=axes[1, 1], title='ACF of Absolute Returns')\n",
    "axes[1, 1].set_xlabel('Lag (days)')\n",
    "axes[1, 1].set_ylabel('Autocorrelation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/acf_pacf_analysis.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Ljung-Box test for autocorrelation\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "lb_test = acorr_ljungbox(coal_returns, lags=[10, 20, 30], return_df=True)\n",
    "print(\"\\nLjung-Box Test for Autocorrelation in Returns:\")\n",
    "print(lb_test)\n",
    "\n",
    "lb_test_sq = acorr_ljungbox(coal_returns**2, lags=[10, 20, 30], return_df=True)\n",
    "print(\"\\nLjung-Box Test for Autocorrelation in Squared Returns (ARCH effects):\")\n",
    "print(lb_test_sq)\n",
    "\n",
    "print(\"\\n✓ Key Finding: Returns show weak autocorrelation, but squared returns\")\n",
    "print(\"  show strong autocorrelation → volatility clustering exists (ARCH effects)\")\n",
    "print(\"\\n✓ ACF/PACF analysis saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba15c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SPREAD VARIABLES (Price Relationships)\n",
    "# =============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"SPREAD VARIABLE CREATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create spread features (differences in returns capture relative movements)\n",
    "spread_features = pd.DataFrame(index=df_returns.index)\n",
    "\n",
    "# Coal vs Energy spreads\n",
    "spread_features['spread_coal_brent'] = df_returns['coal_china_yzcm_ret'] - df_returns['brent_crude_ret']\n",
    "spread_features['spread_coal_gas'] = df_returns['coal_china_yzcm_ret'] - df_returns['natural_gas_hh_ret']\n",
    "spread_features['spread_coal_wti'] = df_returns['coal_china_yzcm_ret'] - df_returns['wti_crude_ret']\n",
    "\n",
    "# Energy spreads (crack spreads proxy)\n",
    "spread_features['spread_gas_oil'] = df_returns['natural_gas_hh_ret'] - df_returns['brent_crude_ret']\n",
    "spread_features['spread_brent_wti'] = df_returns['brent_crude_ret'] - df_returns['wti_crude_ret']\n",
    "\n",
    "# Coal vs industrial\n",
    "spread_features['spread_coal_steel'] = df_returns['coal_china_yzcm_ret'] - df_returns['steel_slx_ret']\n",
    "\n",
    "# Create lagged spreads (1, 5, 10 days)\n",
    "for col in ['spread_coal_brent', 'spread_coal_gas', 'spread_gas_oil']:\n",
    "    for lag in [1, 5, 10]:\n",
    "        spread_features[f'{col}_lag{lag}'] = spread_features[col].shift(lag)\n",
    "\n",
    "print(f\"Created {len(spread_features.columns)} spread features:\")\n",
    "for col in spread_features.columns[:6]:\n",
    "    print(f\"  • {col}\")\n",
    "print(f\"  ... and {len(spread_features.columns)-6} more lagged spreads\")\n",
    "\n",
    "# Correlation of spreads with target\n",
    "print(\"\\nCorrelation of Spreads with Next-Day Coal Return:\")\n",
    "target_next = df_returns['coal_china_yzcm_ret'].shift(-1)\n",
    "spread_corrs = spread_features.corrwith(target_next).dropna().sort_values(key=abs, ascending=False)\n",
    "print(spread_corrs.head(10))\n",
    "\n",
    "# Add to feature set\n",
    "print(f\"\\n✓ Spread features created: {spread_features.shape}\")\n",
    "\n",
    "# Plot spread analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Coal-Gas spread over time\n",
    "ax1 = axes[0, 0]\n",
    "spread_features['spread_coal_gas'].dropna().plot(ax=ax1, alpha=0.7)\n",
    "ax1.axhline(y=0, color='r', linestyle='--')\n",
    "ax1.set_title('Coal-Gas Return Spread Over Time')\n",
    "ax1.set_ylabel('Spread (Coal - Gas)')\n",
    "\n",
    "# Coal-Brent spread over time\n",
    "ax2 = axes[0, 1]\n",
    "spread_features['spread_coal_brent'].dropna().plot(ax=ax2, alpha=0.7)\n",
    "ax2.axhline(y=0, color='r', linestyle='--')\n",
    "ax2.set_title('Coal-Brent Return Spread Over Time')\n",
    "ax2.set_ylabel('Spread (Coal - Brent)')\n",
    "\n",
    "# Distribution of coal-gas spread\n",
    "ax3 = axes[1, 0]\n",
    "spread_features['spread_coal_gas'].dropna().hist(bins=50, ax=ax3, alpha=0.7, edgecolor='black')\n",
    "ax3.axvline(x=0, color='r', linestyle='--')\n",
    "ax3.set_title('Distribution of Coal-Gas Spread')\n",
    "ax3.set_xlabel('Spread')\n",
    "\n",
    "# Rolling correlation coal vs gas\n",
    "ax4 = axes[1, 1]\n",
    "rolling_corr = df_returns['coal_china_yzcm_ret'].rolling(63).corr(df_returns['natural_gas_hh_ret'])\n",
    "rolling_corr.plot(ax=ax4, alpha=0.7)\n",
    "ax4.axhline(y=0, color='r', linestyle='--')\n",
    "ax4.set_title('Rolling 63-day Correlation: Coal vs Natural Gas')\n",
    "ax4.set_ylabel('Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/spread_analysis.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Spread analysis saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ddb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RESIDUAL ANALYSIS (Model Diagnostics)\n",
    "# =============================================================================\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox, het_breuschpagan\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import statsmodels.api as sm\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RESIDUAL ANALYSIS (Gradient Boosting Model)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get residuals from best model (Gradient Boosting)\n",
    "residuals = y_test.values - gb_final.predict(X_test)\n",
    "residuals_series = pd.Series(residuals, index=y_test.index)\n",
    "\n",
    "print(f\"\\nResidual Statistics:\")\n",
    "print(f\"  Mean:     {np.mean(residuals):.6f} (should be ~0)\")\n",
    "print(f\"  Std:      {np.std(residuals):.6f}\")\n",
    "print(f\"  Skewness: {pd.Series(residuals).skew():.4f}\")\n",
    "print(f\"  Kurtosis: {pd.Series(residuals).kurtosis():.4f}\")\n",
    "\n",
    "# 1. Test for autocorrelation in residuals\n",
    "print(\"\\n--- Autocorrelation Tests ---\")\n",
    "dw_stat = durbin_watson(residuals)\n",
    "print(f\"Durbin-Watson statistic: {dw_stat:.4f}\")\n",
    "print(f\"  (DW ≈ 2 means no autocorrelation; <1.5 or >2.5 suggests issues)\")\n",
    "\n",
    "lb_resid = acorr_ljungbox(residuals, lags=[5, 10, 20], return_df=True)\n",
    "print(\"\\nLjung-Box Test on Residuals:\")\n",
    "print(lb_resid)\n",
    "print(\"  (p > 0.05 means no significant autocorrelation → Good)\")\n",
    "\n",
    "# 2. Test for heteroskedasticity (ARCH effects)\n",
    "print(\"\\n--- Heteroskedasticity Tests ---\")\n",
    "lb_resid_sq = acorr_ljungbox(residuals**2, lags=[5, 10, 20], return_df=True)\n",
    "print(\"Ljung-Box Test on Squared Residuals (ARCH effects):\")\n",
    "print(lb_resid_sq)\n",
    "\n",
    "# Breusch-Pagan test\n",
    "X_test_const = sm.add_constant(X_test)\n",
    "bp_test = het_breuschpagan(residuals, X_test_const.iloc[:, :10])  # Use first 10 features\n",
    "print(f\"\\nBreusch-Pagan test (first 10 features):\")\n",
    "print(f\"  LM statistic: {bp_test[0]:.4f}\")\n",
    "print(f\"  p-value:      {bp_test[1]:.4f}\")\n",
    "print(f\"  (p > 0.05 means homoskedasticity → Good)\")\n",
    "\n",
    "# 3. Normality test\n",
    "from scipy.stats import jarque_bera, shapiro\n",
    "jb_stat, jb_p = jarque_bera(residuals)\n",
    "print(f\"\\nJarque-Bera Normality Test:\")\n",
    "print(f\"  Statistic: {jb_stat:.4f}, p-value: {jb_p:.4f}\")\n",
    "print(f\"  (p < 0.05 means non-normal residuals - common in finance)\")\n",
    "\n",
    "# Plot residual diagnostics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# 1. Residuals over time\n",
    "ax1 = axes[0, 0]\n",
    "residuals_series.plot(ax=ax1, alpha=0.7)\n",
    "ax1.axhline(y=0, color='r', linestyle='--')\n",
    "ax1.set_title('Residuals Over Time')\n",
    "ax1.set_ylabel('Residual')\n",
    "\n",
    "# 2. Residual distribution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(residuals, bins=50, density=True, alpha=0.7, edgecolor='black')\n",
    "# Overlay normal distribution\n",
    "x_norm = np.linspace(residuals.min(), residuals.max(), 100)\n",
    "ax2.plot(x_norm, stats.norm.pdf(x_norm, np.mean(residuals), np.std(residuals)), 'r-', linewidth=2)\n",
    "ax2.set_title('Residual Distribution vs Normal')\n",
    "ax2.set_xlabel('Residual')\n",
    "\n",
    "# 3. Q-Q plot\n",
    "ax3 = axes[0, 2]\n",
    "stats.probplot(residuals, dist=\"norm\", plot=ax3)\n",
    "ax3.set_title('Q-Q Plot (Normality Check)')\n",
    "\n",
    "# 4. Residuals vs Fitted\n",
    "ax4 = axes[1, 0]\n",
    "fitted = gb_final.predict(X_test)\n",
    "ax4.scatter(fitted, residuals, alpha=0.5, s=10)\n",
    "ax4.axhline(y=0, color='r', linestyle='--')\n",
    "ax4.set_xlabel('Fitted Values')\n",
    "ax4.set_ylabel('Residuals')\n",
    "ax4.set_title('Residuals vs Fitted (Homoskedasticity Check)')\n",
    "\n",
    "# 5. ACF of residuals\n",
    "plot_acf(residuals, lags=30, ax=axes[1, 1], title='ACF of Residuals')\n",
    "\n",
    "# 6. ACF of squared residuals\n",
    "plot_acf(residuals**2, lags=30, ax=axes[1, 2], title='ACF of Squared Residuals (ARCH)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/residual_analysis.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Residual analysis complete!\")\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"  • DW ≈ 2: No significant autocorrelation in residuals\")\n",
    "print(\"  • Some ARCH effects may remain (volatility clustering)\")\n",
    "print(\"  • Non-normal residuals are typical for financial data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad2e3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PERMUTATION FEATURE IMPORTANCE (Model Explainability)\n",
    "# =============================================================================\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PERMUTATION FEATURE IMPORTANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate permutation importance on test set\n",
    "print(\"\\nCalculating permutation importance (this may take a minute)...\")\n",
    "perm_importance = permutation_importance(gb_final, X_test, y_test, \n",
    "                                          n_repeats=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Create DataFrame of results\n",
    "perm_imp_df = pd.DataFrame({\n",
    "    'feature': X_test.columns,\n",
    "    'importance_mean': perm_importance.importances_mean,\n",
    "    'importance_std': perm_importance.importances_std\n",
    "}).sort_values('importance_mean', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Features by Permutation Importance:\")\n",
    "print(\"-\"*60)\n",
    "for i, row in perm_imp_df.head(15).iterrows():\n",
    "    print(f\"  {row['feature']:<40} {row['importance_mean']:.6f} ± {row['importance_std']:.6f}\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Bar plot of top 20 features\n",
    "ax1 = axes[0]\n",
    "top20 = perm_imp_df.head(20)\n",
    "bars = ax1.barh(range(len(top20)), top20['importance_mean'], \n",
    "                xerr=top20['importance_std'], alpha=0.7)\n",
    "ax1.set_yticks(range(len(top20)))\n",
    "ax1.set_yticklabels(top20['feature'], fontsize=9)\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_xlabel('Mean Importance (Decrease in R²)')\n",
    "ax1.set_title('Top 20 Features - Permutation Importance')\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Compare with built-in feature importance\n",
    "ax2 = axes[1]\n",
    "# Get built-in importance\n",
    "builtin_imp = pd.DataFrame({\n",
    "    'feature': X_test.columns,\n",
    "    'builtin': gb_final.feature_importances_\n",
    "}).sort_values('builtin', ascending=False).head(20)\n",
    "\n",
    "# Merge for comparison\n",
    "comparison = builtin_imp.merge(perm_imp_df[['feature', 'importance_mean']], on='feature')\n",
    "comparison = comparison.sort_values('builtin', ascending=False)\n",
    "\n",
    "x = np.arange(len(comparison))\n",
    "width = 0.35\n",
    "ax2.barh(x - width/2, comparison['builtin'], width, label='Built-in (Gini)', alpha=0.7)\n",
    "ax2.barh(x + width/2, comparison['importance_mean'] * 10, width, label='Permutation (×10)', alpha=0.7)\n",
    "ax2.set_yticks(x)\n",
    "ax2.set_yticklabels(comparison['feature'], fontsize=9)\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_xlabel('Importance')\n",
    "ax2.set_title('Built-in vs Permutation Importance')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/permutation_importance.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "perm_imp_df.to_csv('../data/processed/permutation_importance.csv', index=False)\n",
    "\n",
    "print(\"\\n✓ Permutation importance analysis complete!\")\n",
    "print(\"\\nEconomic Interpretation:\")\n",
    "print(\"  • Steel sector has highest impact (coal demand for steel production)\")\n",
    "print(\"  • Energy prices (heating oil, gas) drive substitution effects\")\n",
    "print(\"  • China indicators capture demand-side fundamentals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c6a572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ROLLING Z-SCORE FEATURES\n",
    "# =============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"ROLLING Z-SCORE FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Z-score = (value - rolling_mean) / rolling_std\n",
    "# Indicates how many standard deviations from recent average\n",
    "\n",
    "zscore_features = pd.DataFrame(index=df_returns.index)\n",
    "\n",
    "key_vars = ['coal_china_yzcm_ret', 'brent_crude_ret', 'natural_gas_hh_ret', \n",
    "            'steel_slx_ret', 'china_etf_fxi_ret']\n",
    "\n",
    "for var in key_vars:\n",
    "    if var in df_returns.columns:\n",
    "        for window in [10, 21, 63]:\n",
    "            roll_mean = df_returns[var].rolling(window).mean()\n",
    "            roll_std = df_returns[var].rolling(window).std()\n",
    "            zscore_features[f'{var}_zscore{window}'] = (df_returns[var] - roll_mean) / roll_std\n",
    "\n",
    "print(f\"Created {len(zscore_features.columns)} z-score features:\")\n",
    "for col in zscore_features.columns[:6]:\n",
    "    print(f\"  • {col}\")\n",
    "print(f\"  ... and {len(zscore_features.columns)-6} more\")\n",
    "\n",
    "# Analyze z-score distribution\n",
    "print(f\"\\nZ-Score Statistics (coal_china_yzcm_ret_zscore21):\")\n",
    "zscore_col = 'coal_china_yzcm_ret_zscore21'\n",
    "print(f\"  Mean: {zscore_features[zscore_col].mean():.4f} (should be ~0)\")\n",
    "print(f\"  Std:  {zscore_features[zscore_col].std():.4f} (should be ~1)\")\n",
    "print(f\"  Min:  {zscore_features[zscore_col].min():.4f}\")\n",
    "print(f\"  Max:  {zscore_features[zscore_col].max():.4f}\")\n",
    "\n",
    "# Plot z-scores\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Coal z-score over time\n",
    "ax1 = axes[0, 0]\n",
    "zscore_features['coal_china_yzcm_ret_zscore21'].dropna().plot(ax=ax1, alpha=0.7)\n",
    "ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax1.axhline(y=2, color='r', linestyle='--', alpha=0.5, label='±2 std')\n",
    "ax1.axhline(y=-2, color='r', linestyle='--', alpha=0.5)\n",
    "ax1.set_title('Coal Return Z-Score (21-day window)')\n",
    "ax1.set_ylabel('Z-Score')\n",
    "ax1.legend()\n",
    "\n",
    "# Distribution of z-scores\n",
    "ax2 = axes[0, 1]\n",
    "zscore_features['coal_china_yzcm_ret_zscore21'].dropna().hist(bins=50, ax=ax2, alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(x=0, color='r', linestyle='--')\n",
    "ax2.set_title('Distribution of Coal Z-Scores')\n",
    "ax2.set_xlabel('Z-Score')\n",
    "\n",
    "# Extreme z-scores over time\n",
    "ax3 = axes[1, 0]\n",
    "extreme_up = zscore_features['coal_china_yzcm_ret_zscore21'] > 2\n",
    "extreme_down = zscore_features['coal_china_yzcm_ret_zscore21'] < -2\n",
    "ax3.scatter(zscore_features.index[extreme_up], zscore_features.loc[extreme_up, 'coal_china_yzcm_ret_zscore21'], \n",
    "            c='green', s=20, label=f'Z > 2 ({extreme_up.sum()} days)', alpha=0.7)\n",
    "ax3.scatter(zscore_features.index[extreme_down], zscore_features.loc[extreme_down, 'coal_china_yzcm_ret_zscore21'], \n",
    "            c='red', s=20, label=f'Z < -2 ({extreme_down.sum()} days)', alpha=0.7)\n",
    "ax3.set_title('Extreme Z-Score Events')\n",
    "ax3.legend()\n",
    "ax3.set_ylabel('Z-Score')\n",
    "\n",
    "# Correlation with next-day return\n",
    "ax4 = axes[1, 1]\n",
    "next_return = df_returns['coal_china_yzcm_ret'].shift(-1)\n",
    "zscore_corrs = zscore_features.corrwith(next_return).dropna().sort_values()\n",
    "zscore_corrs.plot(kind='barh', ax=ax4, alpha=0.7)\n",
    "ax4.set_title('Z-Score Correlation with Next-Day Return')\n",
    "ax4.set_xlabel('Correlation')\n",
    "ax4.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/zscore_analysis.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Z-score features analysis complete!\")\n",
    "print(f\"  Extreme events (|Z| > 2): {(extreme_up | extreme_down).sum()} days ({(extreme_up | extreme_down).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df98e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SEASONALITY FEATURES\n",
    "# =============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"SEASONALITY FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create seasonality features\n",
    "seasonality_df = pd.DataFrame(index=df_returns.index)\n",
    "\n",
    "# Extract date components\n",
    "seasonality_df['month'] = df_returns.index.month\n",
    "seasonality_df['quarter'] = df_returns.index.quarter\n",
    "seasonality_df['weekday'] = df_returns.index.weekday  # 0=Monday, 4=Friday\n",
    "seasonality_df['day_of_month'] = df_returns.index.day\n",
    "seasonality_df['week_of_year'] = df_returns.index.isocalendar().week.values\n",
    "\n",
    "# Add coal returns for analysis\n",
    "seasonality_df['coal_return'] = df_returns['coal_china_yzcm_ret']\n",
    "\n",
    "print(\"Created seasonality features:\")\n",
    "print(\"  • month (1-12)\")\n",
    "print(\"  • quarter (1-4)\")\n",
    "print(\"  • weekday (0=Mon, 4=Fri)\")\n",
    "print(\"  • day_of_month (1-31)\")\n",
    "print(\"  • week_of_year (1-52)\")\n",
    "\n",
    "# Analyze seasonality patterns\n",
    "print(\"\\n--- Monthly Seasonality ---\")\n",
    "monthly_stats = seasonality_df.groupby('month')['coal_return'].agg(['mean', 'std', 'count'])\n",
    "monthly_stats['mean_pct'] = monthly_stats['mean'] * 100\n",
    "print(monthly_stats[['mean_pct', 'std', 'count']].round(4))\n",
    "\n",
    "print(\"\\n--- Day of Week Effect ---\")\n",
    "weekday_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "weekday_stats = seasonality_df.groupby('weekday')['coal_return'].agg(['mean', 'std', 'count'])\n",
    "weekday_stats.index = weekday_names\n",
    "weekday_stats['mean_pct'] = weekday_stats['mean'] * 100\n",
    "print(weekday_stats[['mean_pct', 'std', 'count']].round(4))\n",
    "\n",
    "# Statistical test for day-of-week effect\n",
    "from scipy.stats import f_oneway\n",
    "weekday_returns = [seasonality_df[seasonality_df['weekday']==i]['coal_return'].dropna() for i in range(5)]\n",
    "f_stat, p_value = f_oneway(*weekday_returns)\n",
    "print(f\"\\nANOVA test for weekday effect: F={f_stat:.2f}, p={p_value:.4f}\")\n",
    "print(\"  Significant weekday effect\" if p_value < 0.05 else \"  No significant weekday effect\")\n",
    "\n",
    "# Plot seasonality\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Monthly returns\n",
    "ax1 = axes[0, 0]\n",
    "monthly_means = seasonality_df.groupby('month')['coal_return'].mean() * 100\n",
    "colors = ['green' if x > 0 else 'red' for x in monthly_means]\n",
    "monthly_means.plot(kind='bar', ax=ax1, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax1.set_title('Average Monthly Return (%)')\n",
    "ax1.set_xlabel('Month')\n",
    "ax1.set_ylabel('Mean Return (%)')\n",
    "ax1.set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'], rotation=45)\n",
    "\n",
    "# Day of week returns\n",
    "ax2 = axes[0, 1]\n",
    "weekday_means = seasonality_df.groupby('weekday')['coal_return'].mean() * 100\n",
    "colors = ['green' if x > 0 else 'red' for x in weekday_means]\n",
    "weekday_means.plot(kind='bar', ax=ax2, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax2.set_title('Average Return by Day of Week (%)')\n",
    "ax2.set_xticklabels(weekday_names, rotation=45)\n",
    "ax2.set_ylabel('Mean Return (%)')\n",
    "\n",
    "# Quarterly boxplot\n",
    "ax3 = axes[1, 0]\n",
    "seasonality_df.boxplot(column='coal_return', by='quarter', ax=ax3)\n",
    "ax3.set_title('Return Distribution by Quarter')\n",
    "ax3.set_xlabel('Quarter')\n",
    "ax3.set_ylabel('Daily Return')\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "\n",
    "# Monthly volatility\n",
    "ax4 = axes[1, 1]\n",
    "monthly_vol = seasonality_df.groupby('month')['coal_return'].std() * np.sqrt(252) * 100\n",
    "monthly_vol.plot(kind='bar', ax=ax4, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax4.set_title('Annualized Volatility by Month (%)')\n",
    "ax4.set_xlabel('Month')\n",
    "ax4.set_ylabel('Volatility (%)')\n",
    "ax4.set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'], rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/seasonality_analysis.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Seasonality analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febf3950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VARIANCE INFLATION FACTOR (VIF) - Multicollinearity Check\n",
    "# =============================================================================\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VARIANCE INFLATION FACTOR (VIF) ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nVIF > 5: Moderate multicollinearity\")\n",
    "print(\"VIF > 10: High multicollinearity (consider removing)\")\n",
    "\n",
    "# Use a subset of features for VIF (too many features makes it slow)\n",
    "# Select original returns only (not lagged) for cleaner analysis\n",
    "vif_vars = [col for col in df_returns.columns if '_ret' in col and 'lag' not in col and 'ma' not in col and 'vol' not in col]\n",
    "vif_data = df_returns[vif_vars].dropna()\n",
    "\n",
    "print(f\"\\nAnalyzing {len(vif_vars)} base return variables...\")\n",
    "\n",
    "# Calculate VIF\n",
    "vif_results = []\n",
    "for i, col in enumerate(vif_data.columns):\n",
    "    try:\n",
    "        vif = variance_inflation_factor(vif_data.values, i)\n",
    "        vif_results.append({'feature': col, 'VIF': vif})\n",
    "    except:\n",
    "        vif_results.append({'feature': col, 'VIF': np.nan})\n",
    "\n",
    "vif_df = pd.DataFrame(vif_results).sort_values('VIF', ascending=False)\n",
    "\n",
    "print(\"\\nVIF Results (sorted by VIF):\")\n",
    "print(\"-\"*50)\n",
    "for _, row in vif_df.iterrows():\n",
    "    status = \"⚠️ HIGH\" if row['VIF'] > 10 else (\"⚠️ Moderate\" if row['VIF'] > 5 else \"✓ OK\")\n",
    "    print(f\"  {row['feature']:<30} VIF: {row['VIF']:>8.2f}  {status}\")\n",
    "\n",
    "# Identify problematic features\n",
    "high_vif = vif_df[vif_df['VIF'] > 10]['feature'].tolist()\n",
    "moderate_vif = vif_df[(vif_df['VIF'] > 5) & (vif_df['VIF'] <= 10)]['feature'].tolist()\n",
    "\n",
    "print(f\"\\n--- Summary ---\")\n",
    "print(f\"High VIF (>10): {len(high_vif)} features\")\n",
    "print(f\"Moderate VIF (5-10): {len(moderate_vif)} features\")\n",
    "print(f\"OK VIF (<5): {len(vif_df) - len(high_vif) - len(moderate_vif)} features\")\n",
    "\n",
    "if high_vif:\n",
    "    print(f\"\\nFeatures to consider removing: {high_vif}\")\n",
    "\n",
    "# Plot VIF\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "colors = ['red' if v > 10 else ('orange' if v > 5 else 'green') for v in vif_df['VIF']]\n",
    "bars = ax.barh(range(len(vif_df)), vif_df['VIF'], color=colors, alpha=0.7)\n",
    "ax.set_yticks(range(len(vif_df)))\n",
    "ax.set_yticklabels(vif_df['feature'], fontsize=9)\n",
    "ax.axvline(x=5, color='orange', linestyle='--', label='VIF=5 (Moderate)')\n",
    "ax.axvline(x=10, color='red', linestyle='--', label='VIF=10 (High)')\n",
    "ax.set_xlabel('Variance Inflation Factor')\n",
    "ax.set_title('VIF Analysis - Multicollinearity Check')\n",
    "ax.legend()\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/vif_analysis.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "vif_df.to_csv('../data/processed/vif_results.csv', index=False)\n",
    "print(\"\\n✓ VIF analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7439e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LASSO AND ELASTICNET REGRESSION\n",
    "# =============================================================================\n",
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LASSO AND ELASTICNET REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Lasso (L1 regularization) - promotes sparsity\n",
    "print(\"\\n--- LASSO Regression (L1) ---\")\n",
    "lasso_alphas = [0.0001, 0.001, 0.01, 0.1]\n",
    "lasso_results = []\n",
    "\n",
    "for alpha in lasso_alphas:\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000, random_state=42)\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    train_pred = lasso.predict(X_train_scaled)\n",
    "    val_pred = lasso.predict(X_val_scaled)\n",
    "    \n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    n_nonzero = np.sum(lasso.coef_ != 0)\n",
    "    \n",
    "    lasso_results.append({\n",
    "        'alpha': alpha, 'train_rmse': train_rmse, 'val_rmse': val_rmse, \n",
    "        'n_features': n_nonzero, 'model': lasso\n",
    "    })\n",
    "    print(f\"  α={alpha:<6}: Val RMSE={val_rmse:.6f}, Non-zero features: {n_nonzero}/{len(X_train.columns)}\")\n",
    "\n",
    "# Best Lasso\n",
    "best_lasso_idx = np.argmin([r['val_rmse'] for r in lasso_results])\n",
    "best_lasso = lasso_results[best_lasso_idx]['model']\n",
    "best_lasso_alpha = lasso_results[best_lasso_idx]['alpha']\n",
    "print(f\"\\n  Best Lasso: α={best_lasso_alpha}\")\n",
    "\n",
    "# ElasticNet (L1 + L2 regularization)\n",
    "print(\"\\n--- ElasticNet Regression (L1 + L2) ---\")\n",
    "enet_params = [(0.001, 0.5), (0.01, 0.5), (0.001, 0.9), (0.01, 0.9)]\n",
    "enet_results = []\n",
    "\n",
    "for alpha, l1_ratio in enet_params:\n",
    "    enet = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=10000, random_state=42)\n",
    "    enet.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    val_pred = enet.predict(X_val_scaled)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    n_nonzero = np.sum(enet.coef_ != 0)\n",
    "    \n",
    "    enet_results.append({\n",
    "        'alpha': alpha, 'l1_ratio': l1_ratio, 'val_rmse': val_rmse,\n",
    "        'n_features': n_nonzero, 'model': enet\n",
    "    })\n",
    "    print(f\"  α={alpha}, L1_ratio={l1_ratio}: Val RMSE={val_rmse:.6f}, Non-zero: {n_nonzero}\")\n",
    "\n",
    "# Best ElasticNet\n",
    "best_enet_idx = np.argmin([r['val_rmse'] for r in enet_results])\n",
    "best_enet = enet_results[best_enet_idx]['model']\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n--- Test Set Evaluation ---\")\n",
    "models_to_eval = {\n",
    "    'Ridge (α=100)': ridge_final,\n",
    "    f'Lasso (α={best_lasso_alpha})': best_lasso,\n",
    "    'ElasticNet': best_enet,\n",
    "    'Gradient Boosting': gb_final\n",
    "}\n",
    "\n",
    "comparison_results = []\n",
    "for name, model in models_to_eval.items():\n",
    "    if 'Ridge' in name or 'Lasso' in name or 'Elastic' in name:\n",
    "        test_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        test_pred = model.predict(X_test)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "    mae = mean_absolute_error(y_test, test_pred)\n",
    "    dir_acc = np.mean(np.sign(test_pred) == np.sign(y_test)) * 100\n",
    "    \n",
    "    comparison_results.append({'Model': name, 'RMSE': rmse, 'MAE': mae, 'Dir_Acc': dir_acc})\n",
    "    print(f\"  {name:<25}: RMSE={rmse:.6f}, Dir Acc={dir_acc:.1f}%\")\n",
    "\n",
    "# Plot coefficient comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Lasso coefficients (non-zero only)\n",
    "ax1 = axes[0]\n",
    "lasso_coef = pd.Series(best_lasso.coef_, index=X_train.columns)\n",
    "lasso_nonzero = lasso_coef[lasso_coef != 0].sort_values(key=abs, ascending=True)\n",
    "if len(lasso_nonzero) > 0:\n",
    "    colors = ['green' if c > 0 else 'red' for c in lasso_nonzero]\n",
    "    lasso_nonzero.tail(20).plot(kind='barh', ax=ax1, color=colors[-20:], alpha=0.7)\n",
    "    ax1.set_title(f'Lasso Non-Zero Coefficients ({len(lasso_nonzero)} features)')\n",
    "    ax1.set_xlabel('Coefficient')\n",
    "    ax1.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "else:\n",
    "    ax1.text(0.5, 0.5, 'All coefficients are zero', ha='center', va='center')\n",
    "    ax1.set_title('Lasso Coefficients')\n",
    "\n",
    "# Model comparison\n",
    "ax2 = axes[1]\n",
    "comp_df = pd.DataFrame(comparison_results)\n",
    "x = np.arange(len(comp_df))\n",
    "width = 0.35\n",
    "ax2.bar(x - width/2, comp_df['RMSE']*100, width, label='RMSE (×100)', alpha=0.7)\n",
    "ax2.bar(x + width/2, comp_df['Dir_Acc']/2, width, label='Dir Acc (/2)', alpha=0.7)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(comp_df['Model'], rotation=45, ha='right')\n",
    "ax2.set_ylabel('Value')\n",
    "ax2.set_title('Model Comparison')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/lasso_elasticnet.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Lasso and ElasticNet analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed70d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE REMOVAL FRAGILITY TEST\n",
    "# =============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE REMOVAL FRAGILITY TEST\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nRemoving top features one by one to test model robustness...\")\n",
    "\n",
    "# Get top 5 features by importance\n",
    "top_features = perm_imp_df.head(5)['feature'].tolist()\n",
    "print(f\"Top 5 features: {top_features}\")\n",
    "\n",
    "# Baseline performance (all features)\n",
    "baseline_pred = gb_final.predict(X_test)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_pred))\n",
    "baseline_dir_acc = np.mean(np.sign(baseline_pred) == np.sign(y_test)) * 100\n",
    "\n",
    "print(f\"\\nBaseline (all features): RMSE={baseline_rmse:.6f}, Dir Acc={baseline_dir_acc:.1f}%\")\n",
    "\n",
    "# Remove features one by one\n",
    "fragility_results = [{'removed': 'None (Baseline)', 'rmse': baseline_rmse, 'dir_acc': baseline_dir_acc, 'rmse_change': 0}]\n",
    "\n",
    "for feature in top_features:\n",
    "    # Create dataset without this feature\n",
    "    X_train_reduced = X_train.drop(columns=[feature])\n",
    "    X_val_reduced = X_val.drop(columns=[feature])\n",
    "    X_test_reduced = X_test.drop(columns=[feature])\n",
    "    \n",
    "    # Train new model\n",
    "    gb_reduced = GradientBoostingRegressor(\n",
    "        n_estimators=50, learning_rate=0.1, max_depth=3, random_state=42\n",
    "    )\n",
    "    gb_reduced.fit(X_train_reduced, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_pred = gb_reduced.predict(X_test_reduced)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "    dir_acc = np.mean(np.sign(test_pred) == np.sign(y_test)) * 100\n",
    "    rmse_change = ((rmse - baseline_rmse) / baseline_rmse) * 100\n",
    "    \n",
    "    fragility_results.append({\n",
    "        'removed': feature, 'rmse': rmse, 'dir_acc': dir_acc, 'rmse_change': rmse_change\n",
    "    })\n",
    "    print(f\"  Remove {feature:<35}: RMSE={rmse:.6f} ({rmse_change:+.1f}%), Dir Acc={dir_acc:.1f}%\")\n",
    "\n",
    "# Remove ALL top 5 features\n",
    "X_train_no_top5 = X_train.drop(columns=top_features)\n",
    "X_test_no_top5 = X_test.drop(columns=top_features)\n",
    "\n",
    "gb_no_top5 = GradientBoostingRegressor(n_estimators=50, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_no_top5.fit(X_train_no_top5, y_train)\n",
    "test_pred_no_top5 = gb_no_top5.predict(X_test_no_top5)\n",
    "rmse_no_top5 = np.sqrt(mean_squared_error(y_test, test_pred_no_top5))\n",
    "dir_acc_no_top5 = np.mean(np.sign(test_pred_no_top5) == np.sign(y_test)) * 100\n",
    "rmse_change_no_top5 = ((rmse_no_top5 - baseline_rmse) / baseline_rmse) * 100\n",
    "\n",
    "fragility_results.append({\n",
    "    'removed': 'ALL TOP 5', 'rmse': rmse_no_top5, 'dir_acc': dir_acc_no_top5, 'rmse_change': rmse_change_no_top5\n",
    "})\n",
    "print(f\"\\n  Remove ALL TOP 5 features: RMSE={rmse_no_top5:.6f} ({rmse_change_no_top5:+.1f}%), Dir Acc={dir_acc_no_top5:.1f}%\")\n",
    "\n",
    "# Plot fragility analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "fragility_df = pd.DataFrame(fragility_results)\n",
    "\n",
    "# RMSE change\n",
    "ax1 = axes[0]\n",
    "colors = ['green' if x <= 0 else 'red' for x in fragility_df['rmse_change']]\n",
    "colors[0] = 'blue'  # Baseline\n",
    "ax1.barh(range(len(fragility_df)), fragility_df['rmse_change'], color=colors, alpha=0.7)\n",
    "ax1.set_yticks(range(len(fragility_df)))\n",
    "ax1.set_yticklabels(fragility_df['removed'], fontsize=9)\n",
    "ax1.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax1.set_xlabel('RMSE Change (%)')\n",
    "ax1.set_title('Impact of Removing Features on RMSE')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Directional accuracy\n",
    "ax2 = axes[1]\n",
    "colors2 = ['red' if x < baseline_dir_acc else 'green' for x in fragility_df['dir_acc']]\n",
    "colors2[0] = 'blue'  # Baseline\n",
    "ax2.barh(range(len(fragility_df)), fragility_df['dir_acc'], color=colors2, alpha=0.7)\n",
    "ax2.set_yticks(range(len(fragility_df)))\n",
    "ax2.set_yticklabels(fragility_df['removed'], fontsize=9)\n",
    "ax2.axvline(x=baseline_dir_acc, color='blue', linestyle='--', label=f'Baseline ({baseline_dir_acc:.1f}%)')\n",
    "ax2.axvline(x=50, color='red', linestyle=':', label='Random (50%)')\n",
    "ax2.set_xlabel('Directional Accuracy (%)')\n",
    "ax2.set_title('Impact of Removing Features on Directional Accuracy')\n",
    "ax2.legend()\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/fragility_analysis.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "fragility_df.to_csv('../data/processed/fragility_results.csv', index=False)\n",
    "\n",
    "print(\"\\n--- Fragility Assessment ---\")\n",
    "max_rmse_increase = fragility_df['rmse_change'].max()\n",
    "if max_rmse_increase < 5:\n",
    "    print(\"✓ Model is ROBUST: No single feature removal causes >5% RMSE increase\")\n",
    "elif max_rmse_increase < 10:\n",
    "    print(\"⚠️ Model has MODERATE fragility: Some features cause 5-10% RMSE increase\")\n",
    "else:\n",
    "    print(\"⚠️ Model is FRAGILE: Removing top features causes >10% RMSE increase\")\n",
    "\n",
    "print(\"\\n✓ Fragility analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99649d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
